{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb6d8c8c-a9d7-4a88-b63e-4231e05bf4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeout\n",
    "import time"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1c36c02d-e040-4467-b582-eedd948798cb",
   "metadata": {},
   "source": [
    "We want to scape data from every season from 2012 to 2022. We need to be thorough in order to give as much data to our machine as possible, \n",
    "giving it the best chance possible to be correct when we test it out over the current 2023 season.\n",
    "\n",
    "Below I want to write out the different tables of data that I believe necessary, and plan out where everything will go so the data cleaning phase \n",
    "is less complex. \n",
    "\n",
    "\n",
    "\n",
    "For each season, we want each team's regular season stats. The season stats are composed of 4 tables: \n",
    "\n",
    "    - teams_standard_batting (Team Batting)\n",
    "    - teams_standard_pitching (Team Pitching)\n",
    "    - team_output (Wins Above Avg By Position)\n",
    "    - teams_standard_fielding (Team Fielding)\n",
    "\n",
    "    *All of these tables can be acquired through \"Share & Export\" as csv or excel files and should be utilized to reduce scrapping workload.\n",
    "\n",
    "    There are 30 teams with data for each season. \n",
    "\n",
    "\n",
    "\n",
    "In addition to each team's stats per season, we need to grab the results of each game played during the \n",
    "regular season of each season from 2012 to 2022. \n",
    "    \n",
    "    This means the data we need to grab is from the schedule & results of each team in each year. \n",
    "\n",
    "    For each team in a season, there are 2 tables to grab:\n",
    "\n",
    "        - results (Season summary table)\n",
    "        - team_schedule (Team Game-By-Game Schedule (exportable))\n",
    "\n",
    "\n",
    "\n",
    "And with all these tables I will make one large data set that has each teams relavant metrics that will be trained to decide:\n",
    "\n",
    "    Will team A beat team B. 1 for yes, 0 for no. \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7ec348f-86bd-4521-88c0-c88a4dbe7405",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We need to grab seasons 2012 through 2022\n",
    "SEASONS = list(range(2012,2023))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6577140b-0a97-4b84-bc32-7e4026953367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEASONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39702655-de2c-47f5-aed5-907de1a8e957",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09d3db03-ed9d-4cfd-a172-cd41aea489b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize where the data tables are going to be stored once they are scrapped.\n",
    "DATA_DIR = \"data\"\n",
    "TEAM_BATTING_DIR = os.path.join(DATA_DIR, \"team_batting\")\n",
    "TEAM_FIELDING_DIR = os.path.join(DATA_DIR, \"team_fielding\")\n",
    "TEAM_PITCHING_DIR = os.path.join(DATA_DIR, \"team_pitching\")\n",
    "TEAM_SCHEDULE_DIR = os.path.join(DATA_DIR, \"team_schedule\")\n",
    "SEASON_SUMMARY_DIR = os.path.join(DATA_DIR, \"season_summary\")\n",
    "SEASON_STATS_DIR = os.path.join(DATA_DIR, \"season_stats\")\n",
    "GAMES_DIR = os.path.join(DATA_DIR, \"games\")\n",
    "WINS_ABOVE_AVG_POSITION_DIR = os.path.join(DATA_DIR, \"wins_above_avg_position\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d55ddb-39f3-447e-8625-461b0c568483",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c6f24123-8c93-43c8-9c02-f4bb6a3059a7",
   "metadata": {},
   "source": [
    "There's going to be two types of URLs used in the scrapping portion of the project:\n",
    "    \n",
    "    - URL_1 is to be used to scrape the batting, fielding, pitching, and wins_above tables.\n",
    "\n",
    "    - URL_2 is to be used to scrape the schedule and season summary for each team in each season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "117231d0-7ca1-4861-94ea-8c9e22832ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Season_Year = \"\"\n",
    "Team_Name = \"\"\n",
    "\n",
    "URL_1 = f\"https://www.baseball-reference.com/leagues/majors/{Season_Year}.shtml\"\n",
    "\n",
    "URL_2 = f\"https://www.baseball-reference.com/teams/{Team_Name}/{Season_Year}-schedule-scores.shtml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0d81e94-2b6f-41a3-95d3-996430d52cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to scrape html from a webpage\n",
    "\n",
    "async def get_html(url, selector, sleep=5, retries=5):\n",
    "    html = None\n",
    "    for i in range(1, retries+1):\n",
    "        time.sleep(sleep * i)\n",
    "        try:\n",
    "            print(f\"Starting scrape. Attempt {i}\")\n",
    "            async with async_playwright() as p:\n",
    "                browser = await p.firefox.launch()\n",
    "                page = await browser.new_page()\n",
    "                await page.goto(url)\n",
    "                print(await page.title())\n",
    "                html = await page.inner_html(selector)\n",
    "        except PlaywrightTimeout:\n",
    "            print(f\"Timeout error on {url}\")\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "720c153f-d5b9-4717-82ca-58578b3792db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrape content from each mlb season stats page\n",
    "\n",
    "async def scrape_season_stats(Seasons):\n",
    "\n",
    "    for season in Seasons:\n",
    "        URL_1 = f\"https://www.baseball-reference.com/leagues/majors/{season}.shtml\"\n",
    "       \n",
    "        save_path = os.path.join(SEASON_STATS_DIR, URL_1.split(\"/\")[-1])\n",
    "    \n",
    "        html = await get_html(URL_1, \"#content\")\n",
    "        with open(save_path, \"w+\") as f:\n",
    "            f.write(html)\n",
    "\n",
    "    return \"Completed\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1bcb16f9-a4a3-4ecc-be97-f91ddd03581c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting scrape. Attempt 1\n",
      "2013 Major League Baseball Team Statistics | Baseball-Reference.com\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Completed'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test single season scrape\n",
    "season1 = [2013]\n",
    "await scrape_season_stats(season1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "206f84cb-5988-441a-ae9b-ed7a77c43c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!playwright install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a5cf40b-ab11-49ac-a74a-b5fd72ab97de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting scrape. Attempt 1\n",
      "Timeout error on https://www.baseball-reference.com/leagues/majors/2019.shtml\n",
      "Starting scrape. Attempt 2\n",
      "Timeout error on https://www.baseball-reference.com/leagues/majors/2019.shtml\n",
      "Starting scrape. Attempt 3\n",
      "2019 Major League Baseball Team Statistics | Baseball-Reference.com\n",
      "Starting scrape. Attempt 1\n",
      "Timeout error on https://www.baseball-reference.com/leagues/majors/2020.shtml\n",
      "Starting scrape. Attempt 2\n",
      "Timeout error on https://www.baseball-reference.com/leagues/majors/2020.shtml\n",
      "Starting scrape. Attempt 3\n",
      "2020 Major League Baseball Team Statistics | Baseball-Reference.com\n",
      "Starting scrape. Attempt 1\n",
      "2021 Major League Baseball Team Statistics | Baseball-Reference.com\n",
      "Starting scrape. Attempt 1\n",
      "Timeout error on https://www.baseball-reference.com/leagues/majors/2022.shtml\n",
      "Starting scrape. Attempt 2\n",
      "2022 Major League Baseball Team Statistics | Baseball-Reference.com\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Completed'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now loop through Seasons list and scrape each seasons data\n",
    "season_list = list(range(2019,2023))\n",
    "await scrape_season_stats(season_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d029985-447f-45d9-a29e-4b5b66cdcef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting scrape. Attempt 1\n",
      "2010 Major League Baseball Team Statistics | Baseball-Reference.com\n",
      "Starting scrape. Attempt 1\n",
      "Timeout error on https://www.baseball-reference.com/leagues/majors/2011.shtml\n",
      "Starting scrape. Attempt 2\n",
      "2011 Major League Baseball Team Statistics | Baseball-Reference.com\n",
      "Starting scrape. Attempt 1\n",
      "Timeout error on https://www.baseball-reference.com/leagues/majors/2012.shtml\n",
      "Starting scrape. Attempt 2\n",
      "Timeout error on https://www.baseball-reference.com/leagues/majors/2012.shtml\n",
      "Starting scrape. Attempt 3\n",
      "Timeout error on https://www.baseball-reference.com/leagues/majors/2012.shtml\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "write() argument must be str, not None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Now loop through Seasons list and scrape each seasons data\u001b[39;00m\n\u001b[1;32m      2\u001b[0m season_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2010\u001b[39m,\u001b[38;5;241m2014\u001b[39m))\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m scrape_season_stats(season_list)\n",
      "Cell \u001b[0;32mIn[11], line 12\u001b[0m, in \u001b[0;36mscrape_season_stats\u001b[0;34m(Seasons)\u001b[0m\n\u001b[1;32m     10\u001b[0m     html \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m get_html(URL_1, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#content\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(save_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw+\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 12\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(html)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: write() argument must be str, not None"
     ]
    }
   ],
   "source": [
    "#Now loop through Seasons list and scrape each seasons data\n",
    "season_list = list(range(2010,2014))\n",
    "await scrape_season_stats(season_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "920491fc-b659-438f-87bb-58809db2c709",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrape content from each mlb game of season. Start with 2012 b/c miami marlines began then\n",
    "Team_LIST = ['NYM', 'NYY', 'OAK', 'PHI', 'PIT', 'SD', \n",
    "             'SF', 'SEA', 'STL', 'TB', 'TEX', 'TOR', 'WAS']\n",
    "\n",
    "\n",
    "# ['ARI', 'ATL', 'BAL', 'BOS', 'CHC', 'CHW', 'CIN', 'CLE', 'COL', 'DET', 'HOU', 'KAN', 'LAA', 'LAD', 'MIA', 'MIL', 'MIN', \n",
    "Season = 2012\n",
    "\n",
    "\n",
    "async def scrape_games(Team_list, Season):\n",
    "    for team in Team_list:\n",
    "        URL_2 = f\"https://www.baseball-reference.com/teams/{team}/{Season}-schedule-scores.shtml\"\n",
    "       \n",
    "        save_path = os.path.join(GAMES_DIR, URL_2.split(\"/\")[-2])\n",
    "    \n",
    "        html = await get_html(URL_2, \"#content\")\n",
    "        with open(save_path, \"w+\") as f:\n",
    "            f.write(html)\n",
    "\n",
    "    return \"Completed\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34069170-0010-4434-83c8-aeab68f4260a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting scrape. Attempt 1\n",
      "Timeout error on https://www.baseball-reference.com/teams/NYM/2012-schedule-scores.shtml\n",
      "Starting scrape. Attempt 2\n",
      "2012 New York Mets Schedule | Baseball-Reference.com\n",
      "Starting scrape. Attempt 1\n",
      "Timeout error on https://www.baseball-reference.com/teams/NYY/2012-schedule-scores.shtml\n",
      "Starting scrape. Attempt 2\n",
      "2012 New York Yankees Schedule | Baseball-Reference.com\n",
      "Starting scrape. Attempt 1\n",
      "Timeout error on https://www.baseball-reference.com/teams/OAK/2012-schedule-scores.shtml\n",
      "Starting scrape. Attempt 2\n",
      "2012 Oakland Athletics Schedule | Baseball-Reference.com\n",
      "Starting scrape. Attempt 1\n",
      "2012 Philadelphia Phillies Schedule | Baseball-Reference.com\n",
      "Starting scrape. Attempt 1\n",
      "Timeout error on https://www.baseball-reference.com/teams/PIT/2012-schedule-scores.shtml\n",
      "Starting scrape. Attempt 2\n",
      "Timeout error on https://www.baseball-reference.com/teams/PIT/2012-schedule-scores.shtml\n",
      "Starting scrape. Attempt 3\n",
      "2012 Pittsburgh Pirates Schedule | Baseball-Reference.com\n",
      "Starting scrape. Attempt 1\n",
      "Timeout error on https://www.baseball-reference.com/teams/SD/2012-schedule-scores.shtml\n",
      "Starting scrape. Attempt 2\n",
      "Timeout error on https://www.baseball-reference.com/teams/SD/2012-schedule-scores.shtml\n",
      "Starting scrape. Attempt 3\n",
      "2012 San Diego Padres Schedule | Baseball-Reference.com\n",
      "Starting scrape. Attempt 1\n",
      "2012 San Francisco Giants Schedule | Baseball-Reference.com\n",
      "Starting scrape. Attempt 1\n",
      "Timeout error on https://www.baseball-reference.com/teams/SEA/2012-schedule-scores.shtml\n",
      "Starting scrape. Attempt 2\n",
      "Timeout error on https://www.baseball-reference.com/teams/SEA/2012-schedule-scores.shtml\n",
      "Starting scrape. Attempt 3\n",
      "2012 Seattle Mariners Schedule | Baseball-Reference.com\n",
      "Starting scrape. Attempt 1\n",
      "Timeout error on https://www.baseball-reference.com/teams/STL/2012-schedule-scores.shtml\n",
      "Starting scrape. Attempt 2\n",
      "Timeout error on https://www.baseball-reference.com/teams/STL/2012-schedule-scores.shtml\n",
      "Starting scrape. Attempt 3\n",
      "Timeout error on https://www.baseball-reference.com/teams/STL/2012-schedule-scores.shtml\n",
      "Starting scrape. Attempt 4\n",
      "2012 St. Louis Cardinals Schedule | Baseball-Reference.com\n",
      "Starting scrape. Attempt 1\n",
      "Timeout error on https://www.baseball-reference.com/teams/TB/2012-schedule-scores.shtml\n",
      "Starting scrape. Attempt 2\n",
      "Timeout error on https://www.baseball-reference.com/teams/TB/2012-schedule-scores.shtml\n",
      "Starting scrape. Attempt 3\n",
      "2012 Tampa Bay Rays Schedule | Baseball-Reference.com\n",
      "Starting scrape. Attempt 1\n",
      "Timeout error on https://www.baseball-reference.com/teams/TEX/2012-schedule-scores.shtml\n",
      "Starting scrape. Attempt 2\n",
      "2012 Texas Rangers Schedule | Baseball-Reference.com\n",
      "Starting scrape. Attempt 1\n",
      "2012 Toronto Blue Jays Schedule | Baseball-Reference.com\n",
      "Starting scrape. Attempt 1\n",
      "Page Not Found (404 error) | Baseball-Reference.com\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Completed'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test single season scrape\n",
    "await scrape_games(Team_LIST, Season)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c727d268-12f2-4377-945a-508ae3a295b1",
   "metadata": {},
   "source": [
    "So now I have a full season of data for the 2012 season. I have this data in a series of html files, so the next step will be to preprocess these files.\n",
    "I want to go through the html and pull out the relavant data tables, and then put that data in an organized file system. I can then use python and SQL to \n",
    "configure training and testing datasets. \n",
    "\n",
    "Once the script to process and organize the data is complete, I will write a systematic process for gather data from X to Y years, processing that data, and saving everything in valuable way for training & testing models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20d2707-31a5-4766-aa87-68eeed6bfe3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
